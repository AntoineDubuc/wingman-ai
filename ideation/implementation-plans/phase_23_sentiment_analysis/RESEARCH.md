# Phase 23: Real-Time Voice Sentiment Analysis ‚Äî Research

> **Deep-Dive Documentation**: See [`./hume-ai/`](./hume-ai/) folder for comprehensive Hume AI technical specs, integration guide, and pricing reference.

## Executive Summary

This research evaluates options for adding real-time voice sentiment/emotion analysis to Wingman AI during live calls. The goal is to detect caller sentiment and emotion to provide better context for suggestions and potentially display emotional cues to users.

### Critical Finding

**Neither Deepgram nor AssemblyAI support real-time streaming sentiment analysis.** Both only offer sentiment analysis for batch/pre-recorded audio. For real-time emotion detection, **Hume AI** emerges as the clear leader.

### Business Case

Contact centers using real-time sentiment analysis report:
- **40-50% reduction** in service interactions
- **20%+ lower** cost to serve
- **30% improvement** in first call resolution
- **25% reduction** in escalations

---

## Option 1: Hume AI (Recommended for Real-Time)

### Overview

Hume AI is purpose-built for emotion detection from voice, face, and text. Founded on 10+ years of academic research, with publications in Nature Human Behaviour. Raised $50M specifically for emotion AI.

### The 48 Emotions Detected

Hume's speech prosody model detects **48 distinct emotional dimensions**:

| Category | Emotions |
|----------|----------|
| **Positive** | Admiration, Adoration, Aesthetic Appreciation, Amusement, Awe, Calmness, Contentment, Excitement, Interest, Joy, Love, Pride, Relief, Romance, Satisfaction, Triumph |
| **Negative** | Anger, Anxiety, Awkwardness, Boredom, Confusion, Disappointment, Disgust, Distress, Doubt, Embarrassment, Envy, Fear, Guilt, Horror, Pain, Sadness, Shame |
| **Neutral/Complex** | Concentration, Contemplation, Craving, Desire, Determination, Empathic Pain, Entrancement, Nostalgia, Realization, Surprise (positive), Surprise (negative), Sympathy, Tiredness |

### How It Works

Hume analyzes **vocal modulations** including:
- Pitch variations and intonation patterns
- Speech rate and rhythm changes
- Pauses and their duration
- Vocal tension or relaxation
- Volume dynamics
- Vocal bursts (sighs, laughs, "hmm"s, "uh"s)

### Technical Specifications

| Spec | Value |
|------|-------|
| **Latency** | ~300ms time to first byte |
| **API Type** | WebSocket (real-time bidirectional) |
| **Response Format** | JSON with emotion scores (0-1 scale) |
| **SDKs** | TypeScript, Python, React, .NET, Swift |
| **Accuracy** | ~80% relative to image-based model; top performance on industry benchmarks |

### Integration Example (TypeScript)

```typescript
import { HumeClient } from 'hume';

const client = new HumeClient({ apiKey: 'YOUR_API_KEY' });

// Connect to EVI WebSocket
const socket = await client.empathicVoice.chat.connect({
  configId: 'your-config-id',
  onMessage: (message) => {
    if (message.type === 'emotion') {
      console.log('Detected emotions:', message.emotions);
      // { joy: 0.72, interest: 0.65, anxiety: 0.12, ... }
    }
  }
});

// Send audio chunks
socket.sendAudioInput(audioBuffer);
```

### Pricing

| Plan | Monthly Cost | Included | Overage |
|------|-------------|----------|---------|
| **Free** | $0 | Limited trial | ‚Äî |
| **Starter** | $3 | Basic access | Pay-as-you-go |
| **Pro** | $70 | 1,200 EVI minutes | $0.06/min |
| **Scale** | $200 | Higher limits | Volume discount |
| **Enterprise** | Custom | Unlimited | Negotiated |

**Pay-as-you-go Expression Measurement:**
- **Audio only**: $0.0639/min (prosody + vocal bursts + emotional language)
- Video + Audio: $0.0828/min
- Text only: $0.00024/word

### Scientific Validation

- **Nature Human Behaviour (2019, 2023)**: Research on speech prosody and vocal bursts
- **ACM Multimedia (2023)**: Peer-reviewed methodology
- **Interspeech 2022, ICASSP 2023**: Conference publications
- **Dataset**: Hume-Prosody ‚Äî 41+ hours from 1,004 speakers across 3 countries

### Strengths

‚úÖ Purpose-built for real-time emotion detection
‚úÖ 48 granular emotion dimensions (not just positive/negative/neutral)
‚úÖ ~300ms latency ‚Äî suitable for live feedback
‚úÖ WebSocket API for streaming integration
‚úÖ Scientifically validated with peer-reviewed research
‚úÖ Detects vocal bursts (sighs, hesitations) that text analysis misses

### Limitations

‚ö†Ô∏è Separate service from transcription (need Deepgram + Hume)
‚ö†Ô∏è Adds ~$0.06/min cost on top of transcription
‚ö†Ô∏è Running two WebSocket streams adds complexity
‚ö†Ô∏è Expression scores are probabilistic, not definitive emotions

### Fit for Wingman: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê EXCELLENT

Best option for real-time emotion detection during calls. Can run in parallel with existing Deepgram transcription.

**Sources:**
- [Hume AI Expression Measurement](https://dev.hume.ai/docs/expression-measurement/overview)
- [Hume Prosody Research](https://www.hume.ai/research)
- [Hume Pricing](https://www.hume.ai/pricing)
- [EVI TypeScript Quickstart](https://dev.hume.ai/docs/empathic-voice-interface-evi/quickstart/typescript)

---

## Option 2: AWS Nova Sonic

### Overview

Amazon Nova Sonic is a speech-to-speech foundation model that unifies speech understanding and generation. It processes audio directly (not text), preserving emotional nuances. Available via Amazon Bedrock.

### Emotion Capabilities

Nova Sonic **understands emotional context** but does NOT return structured emotion data:
- Detects frustration, excitement, hesitation from voice
- Adapts response tone based on detected emotion
- Angry customers get calm responses; excited users get upbeat replies
- Emotions influence behavior, not returned as JSON fields

### Technical Specifications

| Spec | Value |
|------|-------|
| **Latency** | <300ms transcription, <700ms full response |
| **API Type** | WebSocket via Amazon Bedrock |
| **Languages** | English, Spanish, French, Italian, German |
| **Context Window** | Up to 1M tokens (Nova 2 Sonic) |
| **Response Format** | Audio output + text events (no emotion scores) |

### Pricing (Bedrock)

| Token Type | Cost per 1,000 tokens |
|------------|----------------------|
| **Input Speech** | $0.0034 |
| **Output Speech** | $0.0136 |

**Example**: 10 hours/day of conversation ‚âà $7/day

### Architecture Pattern

Nova Sonic is designed as a **complete voice agent replacement**, not a sentiment analysis add-on:

```
Audio In ‚Üí Nova Sonic ‚Üí Audio Out
         (understands + responds)
```

To get sentiment **data**, you'd need to:
1. Use Nova Sonic for conversation
2. Run Nova Lite separately for analytics dashboard
3. Or extract from conversation transcripts post-call

### Strengths

‚úÖ Industry-leading latency (<300ms transcription)
‚úÖ Native emotion understanding in voice interactions
‚úÖ Low cost ($0.0034-$0.0136 per 1K tokens)
‚úÖ Multi-language support (5 languages)
‚úÖ AWS ecosystem integration

### Limitations

‚ùå **No structured sentiment output** ‚Äî emotions understood, not returned
‚ùå Designed as full voice agent, not sentiment add-on
‚ùå Would require replacing current Gemini suggestion pipeline
‚ùå Quotas require AWS account manager for increases
‚ùå 429 throttling errors common with high request frequency

### Fit for Wingman: ‚≠ê‚≠ê POOR FIT

Nova Sonic is overkill for sentiment alone. It's designed to replace the entire voice pipeline, not augment it. Would require architectural overhaul.

**Sources:**
- [AWS Nova Sonic Blog](https://aws.amazon.com/blogs/aws/introducing-amazon-nova-sonic-human-like-voice-conversations-for-generative-ai-applications/)
- [Nova Sonic Technical Report](https://assets.amazon.science/86/bb/4316d28940bd9a719abb28f45aaf/amazon-nova-sonic-technical-report-and-model-card-6-12.pdf)
- [Amazon Nova Pricing](https://aws.amazon.com/nova/pricing/)
- [Bedrock Quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html)

---

## Option 3: Deepgram Sentiment Analysis

### Overview

Deepgram offers sentiment analysis as an add-on to their speech-to-text API. Add `sentiment=true` to get positive/negative/neutral classifications.

### How It Works

```
GET /v1/listen?sentiment=true
```

Returns sentiment at multiple levels:
- Per word
- Per sentence
- Per utterance
- Per paragraph
- Overall transcript average

### Response Format

```json
{
  "sentiment": {
    "segments": [
      {
        "text": "I'm really frustrated with this issue",
        "sentiment": "negative",
        "sentiment_score": -0.72
      }
    ],
    "average": {
      "sentiment": "negative",
      "sentiment_score": -0.45
    }
  }
}
```

**Score range**: -1 (negative) to +1 (positive)
**Threshold**: ¬±0.333 for classification boundary

### Critical Limitation

> ‚ö†Ô∏è **"Sentiment Analysis is unavailable for streaming with Nova models"**

Deepgram sentiment analysis **ONLY works with pre-recorded/batch audio**. It cannot be used during live calls.

### Pricing

Not publicly documented. Part of Deepgram's pay-as-you-go model, likely bundled with transcription costs.

### Languages

**English only** ‚Äî requesting sentiment with other languages triggers a warning.

### Fit for Wingman: ‚≠ê‚≠ê‚≠ê MODERATE (Post-Call Only)

Could be valuable for **post-call analysis** in call summaries, but useless for real-time during calls.

**Potential Use**: After call ends, re-process transcript with `sentiment=true` to include sentiment trends in the call summary.

**Sources:**
- [Deepgram Sentiment Analysis Docs](https://developers.deepgram.com/docs/sentiment-analysis)
- [Deepgram Audio Intelligence](https://deepgram.com/product/audio-intelligence)

---

## Option 4: AssemblyAI

### Overview

AssemblyAI offers sentiment analysis as part of their "Speech Understanding" features, with strong accuracy and comprehensive Audio Intelligence suite.

### Sentiment Output

```json
{
  "sentiment_analysis_results": [
    {
      "text": "I love this product",
      "sentiment": "POSITIVE",
      "confidence": 0.95,
      "start": 1000,
      "end": 2500,
      "speaker": "A"
    }
  ]
}
```

### Real-Time Streaming?

**Unclear/Likely No.** AssemblyAI's streaming documentation does NOT mention sentiment analysis as a supported feature. The streaming API focuses on:
- Real-time transcription
- Noise reduction
- Turn detection
- LLM Gateway integration

Sentiment analysis appears to be **batch-only**, similar to Deepgram.

### Pricing

| Feature | Cost |
|---------|------|
| Streaming STT | $0.47/hour ($0.00013/second) |
| Sentiment Analysis | $0.02/hour of audio |

### Languages

English only (Global, AU, UK, US variants)

### Fit for Wingman: ‚≠ê‚≠ê‚≠ê MODERATE (Post-Call Only)

Same situation as Deepgram ‚Äî useful for post-call analysis but not real-time. Slightly cheaper than Deepgram for sentiment ($0.02/hr).

**Sources:**
- [AssemblyAI Sentiment Analysis](https://www.assemblyai.com/docs/speech-understanding/sentiment-analysis)
- [AssemblyAI Streaming Docs](https://www.assemblyai.com/docs/guides/streaming)
- [AssemblyAI Pricing](https://www.assemblyai.com/pricing)

---

## Option 5: OpenAI Realtime API

### Overview

OpenAI's Realtime API enables low-latency multimodal conversations via WebSocket using GPT-4o. Preserves phonetic features including emotion.

### Emotion Handling

- **Preserves** intonation, emotion, emphasis, pacing in audio
- **Understands** emotional context (e.g., detects if user sounds discouraged)
- **Responds** empathetically based on detected emotion
- **No structured output** ‚Äî emotions influence responses, not returned as data

### Technical Specs

| Spec | Value |
|------|-------|
| **Latency** | <500ms for voice responses |
| **API Type** | WebSocket |
| **Model** | GPT-4o |

### Fit for Wingman: ‚≠ê‚≠ê POOR FIT

Like Nova Sonic, OpenAI Realtime is designed as a complete voice agent, not a sentiment add-on. Would require replacing Gemini pipeline. No structured sentiment output.

**Sources:**
- [OpenAI Realtime API Discussion](https://community.openai.com/t/realtime-api-audio-analysis-capabilities/1252353)

---

## Comparison Matrix

| Feature | Hume AI | Deepgram | AssemblyAI | Nova Sonic | OpenAI Realtime |
|---------|---------|----------|------------|------------|-----------------|
| **Real-time streaming** | ‚úÖ Yes | ‚ùå No | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes |
| **Structured sentiment output** | ‚úÖ 48 emotions | ‚úÖ 3 levels | ‚úÖ 3 levels | ‚ùå No | ‚ùå No |
| **WebSocket API** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **Latency** | ~300ms | N/A (batch) | N/A (batch) | <300ms | <500ms |
| **Emotion granularity** | 48 dimensions | 3 (pos/neg/neut) | 3 (pos/neg/neut) | Contextual | Contextual |
| **Cost (audio/min)** | $0.064 | Unknown | $0.0003 | ~$0.01 | Variable |
| **Add-on friendly** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚ùå Full replacement | ‚ùå Full replacement |
| **Multi-language** | ‚úÖ Yes | ‚ùå English only | ‚ùå English only | ‚úÖ 5 langs | ‚úÖ Yes |
| **Scientific validation** | ‚úÖ Peer-reviewed | ‚ùå No | ‚ùå No | ‚ùå No | ‚ùå No |

---

## Recommended Architecture

### For Real-Time Sentiment During Calls

**Run Hume AI in parallel with existing Deepgram transcription:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Audio Stream                            ‚îÇ
‚îÇ                          ‚îÇ                                  ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ            ‚ñº                           ‚ñº                    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ    Deepgram     ‚îÇ         ‚îÇ    Hume AI      ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  (Transcription)‚îÇ         ‚îÇ (Emotion Detect)‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ            ‚îÇ                           ‚îÇ                    ‚îÇ
‚îÇ            ‚ñº                           ‚ñº                    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ     Gemini      ‚îÇ         ‚îÇ  Emotion Overlay ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  (Suggestions)  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (UI Component)  ‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ            ‚îÇ                                                ‚îÇ
‚îÇ            ‚ñº                                                ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                      ‚îÇ
‚îÇ   ‚îÇ Suggestion Card ‚îÇ                                      ‚îÇ
‚îÇ   ‚îÇ  + Emotion Cue  ‚îÇ                                      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### For Post-Call Analysis

**Add sentiment to call summary using Deepgram or AssemblyAI:**

```
Call Recording ‚Üí Deepgram (sentiment=true) ‚Üí Summary with Sentiment Trends
```

---

## Cost Analysis

### Real-Time Emotion Detection (Hume AI)

| Call Duration | Hume Cost | Current Deepgram | Total Added Cost |
|---------------|-----------|------------------|------------------|
| 15 min call | $0.96 | ~$0.15 | +$0.96 |
| 30 min call | $1.92 | ~$0.30 | +$1.92 |
| 1 hour call | $3.84 | ~$0.60 | +$3.84 |

**Monthly estimate** (100 calls √ó 30 min average):
- Hume AI: ~$192/month additional
- Provides: Real-time 48-emotion detection

### Post-Call Sentiment (Deepgram/AssemblyAI)

| Provider | Cost per Hour | 100 calls (30 min) |
|----------|--------------|-------------------|
| AssemblyAI | $0.02 | $1.00/month |
| Deepgram | Bundled | $0/additional |

---

## Implementation Recommendations

### Phase 1: Post-Call Sentiment (Low Effort)
- Add `sentiment=true` to Deepgram batch processing
- Include sentiment trends in call summary
- **Effort**: 1-2 days
- **Cost**: Minimal (already paying for Deepgram)

### Phase 2: Real-Time Emotion Indicator (Medium Effort)
- Integrate Hume AI WebSocket in parallel
- Display simple emotion indicator in overlay (üòä üòê üòü)
- **Effort**: 1-2 weeks
- **Cost**: ~$0.06/min per call

### Phase 3: Emotion-Aware Suggestions (High Effort)
- Pass Hume emotion data to Gemini
- Adjust suggestions based on detected frustration/interest
- **Effort**: 2-3 weeks
- **Cost**: Same as Phase 2

---

## Open Questions for Product Decision

1. **What emotions matter most?**
   - Just positive/negative/neutral? ‚Üí Post-call with Deepgram
   - Granular (frustration, interest, confusion)? ‚Üí Real-time with Hume

2. **Who should see the sentiment?**
   - User only (help them read the room)
   - Influence suggestions silently
   - Both

3. **Budget tolerance?**
   - $0/month: Post-call Deepgram only
   - ~$200/month: Real-time Hume for power users
   - Variable: Usage-based Hume for all

4. **Acceptable latency?**
   - <500ms for live indicator: Hume works
   - Post-call only: Deepgram/AssemblyAI works

---

## Conclusion

| Use Case | Recommended Solution | Cost |
|----------|---------------------|------|
| **Real-time emotion detection** | Hume AI | $0.064/min |
| **Post-call sentiment analysis** | Deepgram (bundled) | ~$0 |
| **Full voice agent replacement** | AWS Nova Sonic | $0.01/min |

**Primary Recommendation**: Start with **Hume AI** for real-time emotion detection. It's the only solution that provides structured emotion data during live calls, with scientific validation and reasonable latency (~300ms).

**Secondary**: Add Deepgram `sentiment=true` for post-call summaries at near-zero additional cost.
